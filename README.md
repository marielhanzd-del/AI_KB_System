AI Knowledge Base Search and Enrichment

This project is a Retrieval-Augmented Generation (RAG) system built in under 24 hours using FastAPI, FAISS, SentenceTransformers, and Google Gemini.
It allows users to upload documents, embed their contents into a vector database, and query them with context-grounded answers generated by Gemini.

Design Decisions:

FastAPI for backend is chosen for its speed, async support, and automatic interactive docs (/docs). It is lightweight enough to quickly prototype within 24 hours.

FAISS as in-memory vector store provides efficient similarity search(Using IndexFlatL2 for simplicity and speed). Kept it in-memory only to avoid disk persistence complexity.

SentenceTransformers (all-MiniLM-L6-v2) for embedding model chosen for balance between speed and semantic quality. 384-dimension embeddings allow smaller memory footprint while remaining effective.

Chunking & Storage done as text extracted from PDFs and stored as small chunks in DOC_STORE. Each chunk is linked to its FAISS embedding for retrieval.

Gemini Integration done as Gemini takes the retrieved chunks + user question to generate context-aware answers which ensures responses are not generic but grounded in uploaded docs.

Trade-offs (24h Constraint):

No persistence: FAISS index and documents are reset when the server restarts.

Basic chunking: Used simple paragraph-based chunking instead of advanced sliding-window overlap.

Error handling: Minimal â€“ limited retries for PDF parsing and Gemini API errors.

Testing scope: Focused on core upload/query flow rather than unit testing edge cases.

Security: API key is passed via environment variable but lacks full auth/roles support.

Scalability: Designed for demo-level scale (few documents, single user), not production load.

These trade-offs allowed delivering a working end-to-end RAG pipeline within 24 hours.

How to Run:
1. Clone repo & create environment
git clone https://github.com/your-username/ai-kb-system.git
cd ai-kb-system
python3 -m venv venv
source venv/bin/activate   # (Windows: venv\Scripts\activate)

2. Install dependencies
pip install -r requirements.txt

3. Set environment variable
export GOOGLE_API_KEY="your-gemini-api-key"

4. Start server
uvicorn app.main:app --reload


The server runs at: http://127.0.0.1:8000

Interactive Swagger docs: http://127.0.0.1:8000/docs

How to Test:
Upload a document and query the knowledge base.


Improvements to take it to production level:

Add FAISS persistence (save/load index).

Better chunking (sliding windows, semantic splits).

Frontend for upload + query.

API authentication & rate limiting.

Dockerize for easier deployment.
